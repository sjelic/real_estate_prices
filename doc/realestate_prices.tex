\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{biblatex}
\usepackage{multirow}
\usepackage{ltxtable}
\usepackage{threeparttable}


\addbibresource{refs.bib}

\graphicspath{ {./images/} }


\title{An overview of regression methods for real estate price prediction problem with interaction variables}


\author{
 Branislav Bajat \\
  Faculty of Civil Engineering\\
  University of Belgrade\\
  Bulevar Kralja Aleksandra 73,\\
  11000 Belgrade, Serbia\\
  \texttt{branislav.bajat@grf.bg.ac.rs} \\
  %% examples of more authors
  \And
  Slobodan Jelić \\
  Faculty of Civil Engineering\\
  University of Belgrade\\
  Bulevar Kralja Aleksandra 73,\\
  11000 Belgrade, Serbia\\
  \texttt{slobodan.jelic@grf.bg.ac.rs} \\
\And
Milutin Pejović \\
Faculty of Civil Engineering\\
University of Belgrade\\
Bulevar Kralja Aleksandra 73,\\
11000 Belgrade, Serbia\\
\texttt{milutin.pejovic@grf.bg.ac.rs} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
In this paper we evaluate linear regression models for real estate price prediction on dataset 
\end{abstract}


% keywords can be removed
\keywords{Lasso Regression \and ElasticNet regression \and Interaction variables \and kernel methods}


\section{Introduction}
Although deep learning techniques have developed rapidly in the last decade, the application of linear methods in domains where predictors can be selected directly from the feature space remains challenging. The main goal of this paper is to provide an overview and explore the limitations of linear regression methods for real estate price prediction. Linear methods are simple and informative and provide better insight into the dependence of the target on the covariates. Statistical tests for significance of coefficients and estimation of confidence intervals can be performed for these methods.
\section{Data}

  \begin{table}[!ht]
    \caption{Sample table title}
     \centering
     \begin{longtable}{p{0.2\textwidth}p{0.3\textwidth}p{0.15\textwidth}p{0.07\textwidth}p{0.08\textwidth}p{0.1\textwidth}}
       \toprule
       \textbf{Variable}  & \textbf{Description}     & \textbf{Group} & \textbf{Type} \\
       \midrule
        dist\_airport & Prox. (Euclidian distance) to airport & accessability & float \\
        dist\_highway\_entr & Prox. to highway entrance & accessability & float \\
        dist\_main\_roads & Prox. to main city roads & accessability & float \\
        dist\_public\_transport & Prox. to city bus station & accessability & float \\
        dist\_recreation & Prox. to green areas, forest & accessability & float \\
        dist\_regional\_roads & Prox. to regional roads & accessability & float \\
        dist\_river & Prox. to river banks & accessability & float \\
        dist\_schools & Prox. to university facilities & accessability & float \\
        construct\_age & Number of days after construction & age & float \\
        facade\_age & Number of days after the facade reconstruction & age & float \\
        inst\_age & Number of days after the instalation replacement & age & float \\
        roof\_age & Number of days after the roof reconstruction & age & float \\
        windows\_age & Number of days after the windows replacement & age & float \\
        dist\_highway & Prox. to highway lane & environmental & float \\
        dist\_railway & Prox. to railway & environmental & float \\
        elevation & Elevation above sea level & environmental & float \\
        easting & E coordinate (mathematical) & neighborhood & float \\
        id\_building & Building ID within cadas. community & neighborhood & int \\
        id\_cadas\_com & Unique cadastral community ID & neighborhood & int \\
        northing & N coordinate (mathematical) & neighborhood & float \\
        constr\_type & Construction type (brick, concrete, wood) & structural & str \\
        duplex & Duplex - apartment in two floors (Yes/No) & structural & str \\
        elevator & Elevator (Yes/No) & structural & str \\
        floor\_above\_ground & Apartment above ground floor & structural & int \\
        floor\_appartment & Apartment floor number & structural & int \\
        floor\_entrance & Building entrance floor number & structural & int \\
        floors\_total & Total number of floors in the building & structural & int \\
        house\_type & Housing type (single, double, raw) & structural & str \\
        living\_area & Apartment living area & structural & float \\
        no\_appart & Number of apartments in building & structural & int \\
        no\_rooms & Number of rooms in apartment & structural & int \\
        postion\_type & Position in building (basement, ground, middle, penthouse) & structural & str \\
        total\_area & Apartment total area & structural & float \\
        price\_m2 & Price per m2 & target & float \\
       \bottomrule
     \end{longtable}
     \label{tab:table}
   \end{table}


   Authors in \cite{ceh_estimating_2018} report high multicollinearity that might increase the variance of coefficient estimates and makes the model sensitive to small data changes. On the other hand, elastic net approach combines shrinking stratzegies to minimize the variance of coefficient estimates and maximize sparseness at the same time.


\section{Models}
\section{Linear regression}
Linear regression is a fundamental method and a common building block of other machine learning algorithms in which a random variable $Y$ (i.e., the target) is expressed as a linear combination of random variables $X_i$, $i\in\{1,\ldots, d\}$, called covariates. In this paper, we use linear regression with basis function expansion under the following assumptions:

\begin{itemize}
  \item the target $Y$ is equal to a linear combination of $X_i$, $i\in\{1,\ldots, d\}$, plus a (random) residual $\varepsilon$, i.e.
  \begin{equation}
    Y = \sum_{i=1}^d w_i \phi(X_i) + \varepsilon,
  \end{equation}
  \item residual $$\varepsilon = Y - \sum_{i=1}^d w_iX_i $$
  is normally distributed with mean zero and constant variance $\sigma^2$, i.e. $\varepsilon\sim \mathcal{N}(0,\sigma^2)$ where $w_i\in\mathbb{R}$, $i\in\{1,\ldots d\}$, $d\in \mathbb{N}$.
\end{itemize}



\subsection{Ridge Regression}
\subsection{Lasso Regression}
Although we do not start with a large number of predictors to use LASSo as feature selector and predictor at the same time, in some approaches we create a large number of predictors by introducing new dicretisized and interaction variables. In the case of large number of predictors (i.e. covariates) we want to select the most appropriate subset of feature. LASSO \cite{tibshiraniRegressionShrinkageSelection1996} <(least absolute shrinkage and selection operator) is a regression method where mean squared error and penalized sum of absolute values of regression parameters is minimized



The solution of \ref{eq:lasso_obj}
Usually, when we want a sparse solution in case of high dimensional data, lasso regression is applied. Here, we assume that parameters $w_j$, $j\in {0,\ldots, d}$ come from Laplacian zero-mean distribution where the most of probability mass is put around zero. In a such way, we aim to select sparse solution. Based on that idea, estimators for lasso parameters are obtained by solving optimization problem were we want to penalize objective function (i.e. mean squared error) regularized with the $l_1$-norm of vector of parameters as follows:

\begin{equation}\label{eq:lasso_obj}
  \min_{w\in \mathbb{R}^{d+1}} \sum_{i=1}^N(y_i-w_i^Tx_i)^2 + \lambda\sum_{i=1}^{d+1}|w_i|,
\end{equation}

for some $\lambda>0$.


This objective loss yields sparse solution where all features with $w_i>0$ are selected. Obviously, this apporach incorporates model selection into regression problem.

\subsection{ElasticNet Regression}

\subsection{AdaBoost Lasso Regression}

\subsection{RandomForest Regression}

\subsection{Kernel Ridge Regression}



\section{Interaction models}

\subsection{Machine learning pipeline}

\begin{figure}
  \includegraphics[width=0.45\textwidth]{ml_pipeline.pdf}
\end{figure}

Continues features from original feature space are discretized to $3$ bins using one-hot encoding technique with quantile strategy. At the second step,  

\subsection{Interpretation}



\section{Results}


\subsection{Experiments}

\subsection{Model selection}
In the  previous section we found that the elastic net outperformed other approaches that used regularization and model selection procedures. In the seminal papar of Zou and Hastie elastic net approach is presented as  a new regularization and model selection method. This approach compromises between ridge and lasso regressions, that serve as regularization and model selection approaches. Since lasso tends to select one feature from group of correlated features, elastic net approach tends to include or exclude the entire group of correlated features. Our statistical analysis in this section uses lasso as model selection procedure 

\subsection{Analysis}
\newpage
\subsection{Statistical analysis of regression coefficients}


% Please add the following required packages to your document preamble:

% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
\def\arraystretch{1.8}
\scriptsize{
\begin{longtable}[c]{|p{2.3cm}|ll|ll|ll|ll|ll|ll|}
  \hline
  \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Method}}}      & \multicolumn{2}{c|}{$r^2$}            & \multicolumn{2}{c|}{\textbf{RMSE}}              & \multicolumn{2}{c|}{\textbf{COD}}           & \multicolumn{2}{c|}{\textbf{MAPE}}          & \multicolumn{2}{c|}{\textbf{MAE}}               & \multicolumn{2}{c|}{\textbf{MPE}}             \\ \cline{2-13} 
  \multicolumn{1}{|c|}{}                             & \multicolumn{1}{l|}{TRAIN} & TEST  & \multicolumn{1}{l|}{TRAIN}   & TEST    & \multicolumn{1}{l|}{TRAIN} & TEST  & \multicolumn{1}{l|}{TRAIN} & TEST  & \multicolumn{1}{l|}{TRAIN}   & TEST    & \multicolumn{1}{l|}{TRAIN}  & TEST   \\ \hline
  \endhead
  %
  Ridge                 & \multicolumn{1}{l|}{0.344} & 0.329 & \multicolumn{1}{l|}{478.959} & 483.544 & \multicolumn{1}{l|}{0.149} & 0.151 & \multicolumn{1}{l|}{0.148} & 0.150 & \multicolumn{1}{l|}{352.127} & 353.801 & \multicolumn{1}{l|}{0.002}  & 0.004  \\
  Regression& \multicolumn{1}{l|}{0.009} & 0.021 & \multicolumn{1}{l|}{3.471}   & 8.412   & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{1.861}   & 4.530   & \multicolumn{1}{l|}{0.000}  & 0.003  \\ \hline
  Lasso                  & \multicolumn{1}{l|}{0.341} & 0.328 & \multicolumn{1}{l|}{480.046} & 483.951 & \multicolumn{1}{l|}{0.149} & 0.151 & \multicolumn{1}{l|}{0.148} & 0.150 & \multicolumn{1}{l|}{352.236} & 353.943 & \multicolumn{1}{l|}{0.001}  & 0.003  \\
  Regression & \multicolumn{1}{l|}{0.009} & 0.021 & \multicolumn{1}{l|}{3.577}   & 8.518   & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{1.866}   & 4.583   & \multicolumn{1}{l|}{0.000}  & 0.003  \\ \hline
  ElasticNet            & \multicolumn{1}{l|}{0.344} & 0.329 & \multicolumn{1}{l|}{479.010} & 483.565 & \multicolumn{1}{l|}{0.149} & 0.151 & \multicolumn{1}{l|}{0.148} & 0.150 & \multicolumn{1}{l|}{352.236} & 353.943 & \multicolumn{1}{l|}{0.002}  & 0.004  \\
  Regression& \multicolumn{1}{l|}{0.009} & 0.021 & \multicolumn{1}{l|}{3.472}   & 8.451   & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{1.866}   & 4.583   & \multicolumn{1}{l|}{0.000}  & 0.003  \\ \hline
  AdaBoost Lasso & \multicolumn{1}{l|}{0.374} & 0.359 & \multicolumn{1}{l|}{467.605} & 472.499 & \multicolumn{1}{l|}{0.143} & 0.144 & \multicolumn{1}{l|}{0.142} & 0.143 & \multicolumn{1}{l|}{341.793} & 344.515 & \multicolumn{1}{l|}{-0.001} & -0.001 \\
  Regression & \multicolumn{1}{l|}{0.008} & 0.017 & \multicolumn{1}{l|}{3.189}   & 7.240   & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{1.736}   & 3.904   & \multicolumn{1}{l|}{0.001}  & 0.003  \\ \hline
  RandomForest & \multicolumn{1}{l|}{0.801} & \textbf{0.440} & \multicolumn{1}{l|}{263.812} & 441.569 & \multicolumn{1}{l|}{0.077} & 0.132 & \multicolumn{1}{l|}{0.077} & 0.132 & \multicolumn{1}{l|}{183.470} & 317.741 & \multicolumn{1}{l|}{-0.008} & -0.001 \\
  Regression & \multicolumn{1}{l|}{0.004} & 0.019 & \multicolumn{1}{l|}{2.918}   & 8.391   & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{1.474}   & 5.169   & \multicolumn{1}{l|}{0.001}  & 0.004  \\ \hline
  Gradient Boost & \multicolumn{1}{l|}{0.669} & \textbf{0.437} & \multicolumn{1}{l|}{339.904} & 443.019 & \multicolumn{1}{l|}{0.106} & 0.132 & \multicolumn{1}{l|}{0.106} & 0.133 & \multicolumn{1}{l|}{254.177} & 319.555 & \multicolumn{1}{l|}{-0.005} & 0.000  \\
  Regression & \multicolumn{1}{l|}{0.005} & 0.021 & \multicolumn{1}{l|}{2.691}   & 8.950   & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{1.929}   & 5.439   & \multicolumn{1}{l|}{0.000}  & 0.004  \\ \hline
  Kernel Ridge & \multicolumn{1}{l|}{0.522} & 0.407 & \multicolumn{1}{l|}{408.674} & 454.338 & \multicolumn{1}{l|}{0.123} & 0.137 & \multicolumn{1}{l|}{0.123} & 0.137 & \multicolumn{1}{l|}{297.548} & 329.601 & \multicolumn{1}{l|}{-0.002} & 0.000  \\
  Regression & \multicolumn{1}{l|}{0.007} & 0.019 & \multicolumn{1}{l|}{3.107}   & 7.829   & \multicolumn{1}{l|}{0.001} & 0.003 & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{1.944}   & 3.902   & \multicolumn{1}{l|}{0.000}  & 0.005  \\ \hline
  Ridge Interaction & \multicolumn{1}{l|}{0.720} & 0.409 & \multicolumn{1}{l|}{312.595} & 453.796 & \multicolumn{1}{l|}{0.095} & 0.138 & \multicolumn{1}{l|}{0.096} & 0.138 & \multicolumn{1}{l|}{228.302} & 331.921 & \multicolumn{1}{l|}{-0.006} & 0.000  \\
  Regression & \multicolumn{1}{l|}{0.004} & 0.014 & \multicolumn{1}{l|}{2.342}   & 6.026   & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{1.836}   & 3.944   & \multicolumn{1}{l|}{0.000}  & 0.004  \\ \hline
  ElasticNet Interaction & \multicolumn{1}{l|}{0.618} & \textbf{0.428} & \multicolumn{1}{l|}{365.469} & 446.385 & \multicolumn{1}{l|}{0.111} & 0.134 & \multicolumn{1}{l|}{0.111} & 0.134 & \multicolumn{1}{l|}{267.477} & 324.818 & \multicolumn{1}{l|}{-0.006} & -0.003 \\
  Regression & \multicolumn{1}{l|}{0.004} & 0.012 & \multicolumn{1}{l|}{2.311}   & 5.555   & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{0.001} & 0.001 & \multicolumn{1}{l|}{1.798}   & 3.098   & \multicolumn{1}{l|}{0.000}  & 0.004  \\ \hline
  Lasso Interaction & \multicolumn{1}{l|}{0.634} & 0.415 & \multicolumn{1}{l|}{357.560} & 451.419 & \multicolumn{1}{l|}{0.110} & 0.136 & \multicolumn{1}{l|}{0.110} & 0.137 & \multicolumn{1}{l|}{263.422} & 329.336 & \multicolumn{1}{l|}{-0.005} & -0.001 \\
  Regression & \multicolumn{1}{l|}{0.004} & 0.015 & \multicolumn{1}{l|}{2.147}   & 6.251   & \multicolumn{1}{l|}{0.001} & 0.002 & \multicolumn{1}{l|}{0.001} & 0.001 & \multicolumn{1}{l|}{1.640}   & 3.885   & \multicolumn{1}{l|}{0.000}  & 0.004  \\ \hline
  \caption{}
  \label{tab:my-table}\\
  \end{longtable}}

  \section{Conclusion}


\printbibliography





\end{document}
