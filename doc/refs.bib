@article{berkValidPostselectionInference2013,
  title = {Valid Post-Selection Inference},
  author = {Berk, Richard and Brown, Lawrence and Buja, Andreas and Zhang, Kai and Zhao, Linda},
  date = {2013-04-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {41},
  number = {2},
  issn = {0090-5364},
  doi = {10.1214/12-AOS1077},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-41/issue-2/Valid-post-selection-inference/10.1214/12-AOS1077.full},
  urldate = {2022-11-02},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\6AMXBM8B\Berk et al. - 2013 - Valid post-selection inference.pdf}
}

@article{cehEstimatingPerformanceRandom2018,
  title = {Estimating the {{Performance}} of {{Random Forest}} versus {{Multiple Regression}} for {{Predicting Prices}} of the {{Apartments}}},
  author = {Čeh, Marjan and Kilibarda, Milan and Lisec, Anka and Bajat, Branislav},
  date = {2018-05-02},
  journaltitle = {ISPRS International Journal of Geo-Information},
  shortjournal = {IJGI},
  volume = {7},
  number = {5},
  pages = {168},
  issn = {2220-9964},
  doi = {10.3390/ijgi7050168},
  url = {http://www.mdpi.com/2220-9964/7/5/168},
  urldate = {2022-10-16},
  abstract = {The goal of this study is to analyse the predictive performance of the random forest machine learning technique in comparison to commonly used hedonic models based on multiple regression for the prediction of apartment prices. A data set that includes 7407 records of apartment transactions referring to real estate sales from 2008–2013 in the city of Ljubljana, the capital of Slovenia, was used in order to test and compare the predictive performances of both models. Apparent challenges faced during modelling included (1) the non-linear nature of the prediction assignment task; (2) input data being based on transactions occurring over a period of great price changes in Ljubljana whereby a 28\% decline was noted in six consecutive testing years; and (3) the complex urban form of the case study area. Available explanatory variables, organised as a Geographic Information Systems (GIS) ready dataset, including the structural and age characteristics of the apartments as well as environmental and neighbourhood information were considered in the modelling procedure. All performance measures (R2 values, sales ratios, mean average percentage error (MAPE), coefficient of dispersion (COD)) revealed significantly better results for predictions obtained by the random forest method, which confirms the prospective of this machine learning technique on apartment price prediction.},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\DW4RRMWQ\Čeh et al. - 2018 - Estimating the Performance of Random Forest versus.pdf}
}

@article{hennigModelsMethodsClusterwise1998,
  title = {Models {{And Methods For Clusterwise Linear Regression}}},
  author = {Hennig, Christian},
  date = {1998-12-19},
  issn = {978-3-540-65855-9},
  doi = {10.1007/978-3-642-60187-3_17},
  abstract = {: Three models for linear regression clustering are given, and corresponding methods for classification and parameter estimation are developed and discussed: The mixture model with fixed regressors (ML-estimation), the fixed partition model with fixed regressors (ML-estimation), and the mixture model with random regressors (Fixed Point Clustering). The number of clusters is treated as unknown. The approaches are compared via an application to Fisher's Iris data. By the way, a broadly ignored feature of these data is discovered. 1 Introduction Cluster analysis problems based on stochastic models can be divided into two classes: 1. A cluster is considered as a subset of the data points, which can be modeled adequately by a distribution from a class of cluster reference distributions (c.r.d.). These distributions are chosen to reflect the meaning of homogeneity with respect to the certain data analysis problem. Therefore c.r.d. are often unimodal. If the class of c.r.d. is parametric, th...},
  file = {C:\Users\sjelic\Zotero\storage\DEXW6I5W\Hennig - 1998 - Models And Methods For Clusterwise Linear Regressi.pdf}
}

@article{knightAsymptoticsLassoTypeEstimators2000,
  title = {Asymptotics for {{Lasso-Type Estimators}}},
  author = {Knight, Keith and Fu, Wenjiang},
  date = {2000},
  journaltitle = {The Annals of Statistics},
  volume = {28},
  number = {5},
  eprint = {2674097},
  eprinttype = {jstor},
  pages = {1356--1378},
  url = {http://www.jstor.org/stable/2674097},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\XK85RRU7\Knight and Fu - 2000 - Asymptotics for Lasso-Type Estimators.pdf}
}

@article{leeExactPostselectionInference2016,
  title = {Exact Post-Selection Inference, with Application to the Lasso},
  author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
  date = {2016-06-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {44},
  number = {3},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1371},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-3/Exact-post-selection-inference-with-application-to-the-lasso/10.1214/15-AOS1371.full},
  urldate = {2022-11-02},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\YW3Y26VJ\Lee et al. - 2016 - Exact post-selection inference, with application t.pdf}
}

@article{liClusterwiseFunctionalLinear2021,
  title = {Clusterwise Functional Linear Regression Models},
  author = {Li, Ting and Song, Xinyuan and Zhang, Yingying and Zhu, Hongtu and Zhu, Zhongyi},
  date = {2021-06-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {158},
  pages = {107192},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2021.107192},
  url = {https://www.sciencedirect.com/science/article/pii/S0167947321000268},
  urldate = {2022-12-16},
  abstract = {Classical clusterwise linear regression is a useful method for investigating the relationship between scalar predictors and scalar responses with heterogeneous variation of regression patterns for different subgroups of subjects. This paper extends the classical clusterwise linear regression to incorporate multiple functional predictors by representing the functional coefficients in terms of a functional principal component basis. We estimate the functional principal component coefficients based on M-estimation and K-means clustering algorithm, which can classify the data into clusters and estimate clusterwise coefficients simultaneously. One advantage of the proposed method is that it is robust and flexible by adopting a general loss function, which can be broadly applied to mean regression, median regression, quantile regression and robust mean regression. A Bayesian information criterion is proposed to select the unknown number of groups and shown to be consistent in model selection. We also obtain the convergence rate of the set of estimators to the set of true coefficients for all clusters. Simulation studies and real data analysis show that the proposed method is easily implemented, and it consequently improves previous works and also requires much less computing burden than existing methods.},
  langid = {english},
  keywords = {Bayesian information criterion consistency,M-estimation,Subgroup analysis},
  file = {C\:\\Users\\sjelic\\Zotero\\storage\\4EIU6Q6T\\Li et al. - 2021 - Clusterwise functional linear regression models.pdf;C\:\\Users\\sjelic\\Zotero\\storage\\I663T4V6\\S0167947321000268.html}
}

@article{taylorPostselectionInferenceL1penalized2018,
  title = {Post-Selection Inference for ℓ1-Penalized Likelihood Models},
  author = {Taylor, Jonathan and Tibshirani, Robert},
  date = {2018-03},
  journaltitle = {Canadian Journal of Statistics},
  shortjournal = {Can. J. Statistics},
  volume = {46},
  number = {1},
  pages = {41--61},
  issn = {03195724},
  doi = {10.1002/cjs.11313},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cjs.11313},
  urldate = {2022-11-02},
  abstract = {We present a new method for post-selection inference for ℓ1 (lasso)-penalized likelihood models, including generalized regression models. Our approach generalizes the post-selection framework presented in Lee et al. (2013). The method provides p-values and confidence intervals that are asymptotically valid, conditional on the inherent selection done by the lasso. We present applications of this work to (regularized) logistic regression, Cox's proportional hazards model and the graphical lasso. We do not provide rigorous proofs here of the claimed results, but rather conceptual and theoretical sketches.},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\BFRBMITG\Taylor and Tibshirani - 2018 - Post-selection inference for ℓ1-penalized likeliho.pdf}
}

@article{tibshiraniRegressionShrinkageSelection1996,
  title = {Regression {{Shrinkage}} and {{Selection Via}} the {{Lasso}}},
  author = {Tibshirani, Robert},
  date = {1996},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {58},
  number = {1},
  pages = {267--288},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1996.tb02080.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
  urldate = {2023-10-04},
  abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  langid = {english},
  keywords = {quadratic programming,regression,shrinkage,subset selection},
  file = {C\:\\Users\\sjelic\\Zotero\\storage\\CNGQ6H6A\\Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf;C\:\\Users\\sjelic\\Zotero\\storage\\G8WE4CYE\\j.2517-6161.1996.tb02080.html}
}

@article{zhangPostmodelselectionInferenceLinear2022,
  title = {Post-Model-Selection Inference in Linear Regression Models: {{An}} Integrated Review},
  shorttitle = {Post-Model-Selection Inference in Linear Regression Models},
  author = {Zhang, Dongliang and Khalili, Abbas and Asgharian, Masoud},
  date = {2022-01-01},
  journaltitle = {Statistics Surveys},
  shortjournal = {Statist. Surv.},
  volume = {16},
  issn = {1935-7516},
  doi = {10.1214/22-SS135},
  url = {https://projecteuclid.org/journals/statistics-surveys/volume-16/issue-none/Post-model-selection-inference-in-linear-regression-models--An/10.1214/22-SS135.full},
  urldate = {2022-11-27},
  abstract = {The research on statistical inference after data-driven model selection can be traced as far back as Koopmans (1949). The intensive research on modern model selection methods for high-dimensional data over the past three decades revived the interest in statistical inference after model selection. In recent years, there has been a surge of articles on statistical inference after model selection and now a rather vast literature exists on this topic. Our manuscript aims at presenting a holistic review of post-model-selection inference in linear regression models, while also incorporating perspectives from high-dimensional inference in these models. We first give a simulated example motivating the necessity for valid statistical inference after model selection. We then provide theoretical insights explaining the phenomena observed in the example. This is done through a literature survey on the post-selection sampling distribution of regression parameter estimators and properties of coverage probabilities of na¨ıve confidence intervals. Categorized according to two types of estimation targets, namely the population- and projection-based regression coefficients, we present a review of recent uncertainty assessment methods. We also discuss possible pros and cons for the confidence intervals constructed by different methods.},
  issue = {none},
  langid = {english},
  file = {C\:\\Users\\sjelic\\Zotero\\storage\\AU2HWSF9\\Zhang et al. - 2022 - Post-model-selection inference in linear regressio.pdf;C\:\\Users\\sjelic\\Zotero\\storage\\AYXZ8H8S\\ss135supp.pdf}
}

@article{zouRegularizationVariableSelection2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  date = {2005-04},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J Royal Statistical Soc B},
  volume = {67},
  number = {2},
  pages = {301--320},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
  urldate = {2022-11-23},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  langid = {english},
  file = {C:\Users\sjelic\Zotero\storage\ZLC5D4AW\Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf}
}
